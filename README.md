# Image Classification using CaffeNet

A comprehensive PyTorch implementation of CaffeNet for image classification tasks. CaffeNet is a variant of the famous AlexNet architecture, optimized for the Caffe deep learning framework, known for its efficiency and strong performance on image classification tasks.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ðŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Sample Use Case](#sample-use-case)
- [Dataset Preparation](#dataset-preparation)
- [Training](#training)
- [Inference](#inference)
- [Model Architecture](#model-architecture-details)
- [Configuration](#configuration)
- [Project Structure](#project-structure)
- [Performance Tips](#performance-tips)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

- **Complete CaffeNet Implementation** - Full PyTorch implementation of the CaffeNet architecture
- **Easy to Use** - Simple training and inference scripts with command-line interface
- **Flexible Dataset Support** - Works with any image classification dataset
- **Data Augmentation** - Built-in data augmentation for improved generalization
- **Training Utilities** - Learning rate scheduling, checkpointing, and training history
- **Comprehensive Examples** - Sample use case with fruit classification
- **Production Ready** - Includes inference script for deployment
- **Well Documented** - Extensive documentation and code comments

## ðŸ—ï¸ Architecture

CaffeNet consists of:
- **5 Convolutional Layers** with ReLU activation and Local Response Normalization
- **3 Max Pooling Layers** for spatial dimension reduction
- **3 Fully Connected Layers** for classification
- **Dropout** for regularization (default: 0.5)
- **~60M Parameters** for standard configuration

### Key Differences from AlexNet:
- Optimized for single GPU training
- Modified convolution parameters
- Better suited for smaller datasets

## ðŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (optional, but recommended)

### Step 1: Clone the Repository
```bash
git clone https://github.com/yourusername/Imageclassification_CaffeNet.git
cd Imageclassification_CaffeNet
```

### Step 2: Create Virtual Environment (Recommended)
```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Or using conda
conda create -n caffenet python=3.8
conda activate caffenet
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

## ðŸŽ¯ Quick Start

### 1. Create Sample Dataset
Run the sample use case to create a demonstration fruit classification dataset:

```bash
python examples/sample_use_case.py
```

This creates a dataset with 5 fruit classes: Apple, Banana, Orange, Strawberry, and Grape.

### 2. Train the Model
```bash
python train.py --data-dir data_fruits --epochs 20 --batch-size 16 --save-dir checkpoints_fruits
```

### 3. Run Inference
```bash
# Single image
python predict.py --image-path data_fruits/val/apple/apple_0001.png --checkpoint checkpoints_fruits/best_model.pth

# Directory of images
python predict.py --image-path data_fruits/val --checkpoint checkpoints_fruits/best_model.pth --output predictions.json
```

## ðŸŽ Sample Use Case: Fruit Classification System

### Business Problem
A grocery distribution center needs to automate fruit sorting for:
- Inventory management
- Quality control  
- Automated pricing
- Storage optimization

### Solution
Use CaffeNet to classify incoming fruit images into 5 categories with high accuracy and confidence scores.

### Expected Results
- **Classification Accuracy**: >90% on validation set
- **Inference Time**: <100ms per image
- **Scalability**: Easily extensible to more fruit types

### Running the Use Case
```bash
# Generate sample data and see complete workflow
python examples/sample_use_case.py

# Train on fruit dataset
python train.py --data-dir data_fruits --epochs 20 --batch-size 16 --lr 0.001 --save-dir checkpoints_fruits

# Evaluate
python predict.py --image-path data_fruits/val --checkpoint checkpoints_fruits/best_model.pth --top-k 3
```

## ðŸ“ Dataset Preparation

### Directory Structure
Organize your dataset in the following structure:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”‚   â”œâ”€â”€ image2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ class2/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ classN/
â”‚       â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ class1/
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ class2/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ classN/
        â””â”€â”€ ...
```

### Supported Image Formats
- JPEG (.jpg, .jpeg)
- PNG (.png)
- BMP (.bmp)
- GIF (.gif)

### Recommended Dataset Sizes
- **Minimum**: 100 images per class
- **Recommended**: 500+ images per class
- **Optimal**: 1000+ images per class

### Data Split
- **Training**: 70-80% of data
- **Validation**: 20-30% of data
- **Test** (optional): Hold-out set for final evaluation

## ðŸŽ“ Training

### Basic Training
```bash
python train.py --data-dir data --epochs 50 --batch-size 32
```

### Advanced Training Options
```bash
python train.py \
  --data-dir data \
  --epochs 100 \
  --batch-size 64 \
  --lr 0.001 \
  --momentum 0.9 \
  --weight-decay 5e-4 \
  --dropout 0.5 \
  --scheduler step \
  --step-size 30 \
  --gamma 0.1 \
  --num-workers 8 \
  --save-dir checkpoints \
  --input-size 227
```

### Training Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--data-dir` | Path to dataset directory | `data` |
| `--epochs` | Number of training epochs | `50` |
| `--batch-size` | Batch size for training | `32` |
| `--lr` | Initial learning rate | `0.001` |
| `--momentum` | SGD momentum | `0.9` |
| `--weight-decay` | Weight decay (L2 regularization) | `5e-4` |
| `--dropout` | Dropout probability | `0.5` |
| `--scheduler` | LR scheduler (step/plateau/none) | `step` |
| `--step-size` | Step size for StepLR | `10` |
| `--gamma` | LR decay factor | `0.1` |
| `--num-workers` | Number of data loading workers | `4` |
| `--save-dir` | Directory to save checkpoints | `checkpoints` |
| `--input-size` | Input image size | `227` |

### Monitoring Training

Training progress is saved to `training_history.json`:
```json
{
  "train_loss": [2.3, 1.8, 1.5, ...],
  "train_acc": [45.2, 62.3, 71.8, ...],
  "val_loss": [2.1, 1.7, 1.4, ...],
  "val_acc": [48.5, 64.1, 73.2, ...]
}
```

### Saved Files
- `best_model.pth` - Best model based on validation accuracy
- `checkpoint_epoch_N.pth` - Checkpoint for epoch N
- `class_to_idx.json` - Class name to index mapping
- `training_history.json` - Training and validation metrics

## ðŸ”® Inference

### Single Image Prediction
```bash
python predict.py --image-path path/to/image.jpg --checkpoint checkpoints/best_model.pth
```

### Batch Prediction
```bash
python predict.py --image-path path/to/images/ --checkpoint checkpoints/best_model.pth --output results.json
```

### Inference Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--image-path` | Path to image file or directory | Required |
| `--checkpoint` | Path to model checkpoint | `checkpoints/best_model.pth` |
| `--input-size` | Input image size | `227` |
| `--top-k` | Number of top predictions | `5` |
| `--output` | Path to save results (JSON) | `None` |

### Example Output
```
Image: data_fruits/val/apple/apple_0001.png
Top 5 predictions:
  1. apple: 96.73%
  2. strawberry: 2.14%
  3. orange: 0.89%
  4. grape: 0.15%
  5. banana: 0.09%
```

## ðŸ”¬ Model Architecture Details

### Layer Configuration

```
Input: 3 Ã— 227 Ã— 227

Conv1: 96 filters, 11Ã—11 kernel, stride 4
  â†’ ReLU â†’ LRN â†’ MaxPool (3Ã—3, stride 2)
  Output: 96 Ã— 27 Ã— 27

Conv2: 256 filters, 5Ã—5 kernel, padding 2
  â†’ ReLU â†’ LRN â†’ MaxPool (3Ã—3, stride 2)
  Output: 256 Ã— 13 Ã— 13

Conv3: 384 filters, 3Ã—3 kernel, padding 1
  â†’ ReLU
  Output: 384 Ã— 13 Ã— 13

Conv4: 384 filters, 3Ã—3 kernel, padding 1
  â†’ ReLU
  Output: 384 Ã— 13 Ã— 13

Conv5: 256 filters, 3Ã—3 kernel, padding 1
  â†’ ReLU â†’ MaxPool (3Ã—3, stride 2)
  Output: 256 Ã— 6 Ã— 6

AdaptiveAvgPool: 6 Ã— 6
Flatten: 9216 features

FC1: 9216 â†’ 4096
  â†’ Dropout â†’ ReLU

FC2: 4096 â†’ 4096
  â†’ Dropout â†’ ReLU

FC3: 4096 â†’ num_classes
```

### Testing the Model
```python
from models import caffenet

# Create model
model = caffenet(num_classes=10)

# Test forward pass
import torch
dummy_input = torch.randn(1, 3, 227, 227)
output = model(dummy_input)
print(output.shape)  # torch.Size([1, 10])
```

## âš™ï¸ Configuration

You can use the provided `config.yaml` for configuration management:

```yaml
model:
  num_classes: 1000
  dropout: 0.5
  input_size: 227

training:
  epochs: 50
  learning_rate: 0.001
  momentum: 0.9
  weight_decay: 0.0005
  
# ... more configurations
```

## ðŸ“‚ Project Structure

```
Imageclassification_CaffeNet/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ caffenet.py          # CaffeNet architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py           # Dataset utilities
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ sample_use_case.py   # Fruit classification example
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ predict.py               # Inference script
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ config.yaml              # Configuration file
â””â”€â”€ README.md                # This file
```

## ðŸš€ Performance Tips

### Training Optimization
1. **Use GPU**: Dramatically faster training
   ```bash
   # Check GPU availability
   python -c "import torch; print(torch.cuda.is_available())"
   ```

2. **Increase Batch Size**: If you have enough GPU memory
   ```bash
   python train.py --batch-size 64  # or 128
   ```

3. **More Workers**: Speed up data loading
   ```bash
   python train.py --num-workers 8
   ```

4. **Data Augmentation**: Improve generalization
   - Modify `utils/dataset.py` to add more augmentations
   - Use `albumentations` for advanced augmentation

### Memory Optimization
- Reduce batch size if running out of memory
- Use gradient accumulation for large effective batch sizes
- Consider mixed precision training (requires code modification)

### Better Accuracy
1. **More Data**: Collect more training samples
2. **Data Augmentation**: Increase diversity
3. **Hyperparameter Tuning**: Experiment with learning rate, weight decay
4. **Transfer Learning**: Start from pretrained weights (if available)
5. **Ensemble Methods**: Combine multiple models

## ðŸ”§ Customization

### Adding Your Own Dataset
1. Organize images in the required directory structure
2. Run training with your data directory
3. The system automatically detects classes from folder names

### Modifying the Architecture
Edit `models/caffenet.py`:
```python
# Example: Change dropout rate
model = caffenet(num_classes=10, dropout=0.3)

# Example: Add more layers (requires editing the class)
```

### Custom Data Augmentation
Edit `utils/dataset.py`:
```python
train_transform = transforms.Compose([
    transforms.Resize((input_size, input_size)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
    # Add more augmentations here
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

## ðŸ“Š Results and Benchmarks

### Sample Dataset (Fruits)
- **Dataset Size**: 250 images (50 per class)
- **Classes**: 5 (Apple, Banana, Orange, Strawberry, Grape)
- **Training Accuracy**: ~95%
- **Validation Accuracy**: ~85-90%
- **Training Time**: ~5-10 minutes (GPU)

### Tips for Your Dataset
- Results vary based on dataset complexity
- More data generally leads to better performance
- Consider class balance for optimal results

## ðŸ› Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```bash
# Reduce batch size
python train.py --batch-size 16
```

**2. Slow Training**
```bash
# Increase workers (if you have multiple CPU cores)
python train.py --num-workers 8
```

**3. Poor Accuracy**
- Check if dataset is balanced
- Ensure images are of good quality
- Try different learning rates
- Add more data augmentation
- Train for more epochs

**4. Import Errors**
```bash
# Make sure all dependencies are installed
pip install -r requirements.txt
```

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Setup
```bash
# Clone the repository
git clone https://github.com/yourusername/Imageclassification_CaffeNet.git
cd Imageclassification_CaffeNet

# Install dependencies
pip install -r requirements.txt

# Run tests (if available)
pytest tests/
```

## ðŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ðŸ™ Acknowledgments

- Original CaffeNet architecture from the Caffe deep learning framework
- Inspired by AlexNet (Krizhevsky et al., 2012)
- Built with PyTorch

## ðŸ“§ Contact

For questions, issues, or suggestions:
- Open an issue on GitHub
- Email: your.email@example.com

## ðŸ“š References

1. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks.
2. Jia, Y., et al. (2014). Caffe: Convolutional architecture for fast feature embedding.

## ðŸ—ºï¸ Roadmap

- [ ] Add pretrained weights
- [ ] Support for transfer learning
- [ ] Add more evaluation metrics
- [ ] Implement model visualization tools
- [ ] Add TensorBoard integration
- [ ] Create Docker container
- [ ] Add REST API for inference
- [ ] Mobile deployment support

---

**Happy Classifying! ðŸŽ‰**

If you find this project helpful, please give it a â­!
